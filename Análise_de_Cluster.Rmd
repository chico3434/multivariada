---
title: "Análise Multivariada"
subtitle: "Análise de Cluster"
author: "Francisco Rubens e Julio Cesar"
date: "2023-05-30"
output:
  rmdformats::readthedown:
    highlight: kate
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache=FALSE,
                      prompt=FALSE,
                      tidy=TRUE,
                      comment=NA)
```

```{r libraries}
library(ggplot2)
library(ggpubr)
library(DT)
library(mclust)
library(factoextra)
library(glue)
library(cluster)
library(fpc)
library(ape)
library(purrr)
library(mclust)
```

## Dados

Os dados se referem a uma análise química de vinhos. São 178 amostras de diferentes vinhos italianos de 3 tipos de uvas usadas na fabricação dos vinhos, tal como Sauvignon Blanc, Cabernet ou Chardonnay. Foram analisadas 13 variáveis (V2 a V14), contendo as concentrações de diferentes compostos químicos na amostra.  

As variáveis observadas para cada vinho são:  

V2. Álcool, que é criado como um resultado direto do processo de fermentação e é também um indicador do conteúdo de álcool das uvas.  
V3. Ácido málico, que é um acido primário em uvas, que podem influenciar o gosto dos vinhos.  
V4. Teor de cinzas, que são um indicador de qualidade.  
V5. Alcalinidade da cinza, uma propriedade química das cinzas.  
V6. Magnésio, um mineral.  
V7. Fenóis totais, uma classe de moléculas importantes para definir o sabor, cheiro, benefícios medicinais e diversidade do vinho. Os tipos de fenóis são classificados como flavonóides e não flavonoides.  
V8. Flavonóides, um tipo de fenol no vinho tinto que possui um maior impacto no sabor do vinho.  
V9. Fenóis não flavonóides, outro tipo de fenol.  
V10. Proantocianinas, tipo de flavonóide das semestes das uvas.  
V11. Intensidade da cor.  
V12. Tonalidade do vinho.  
V13. OD280/OD315 de vinhos diluídos  
V14. Teor de prolina, que é alterada pela variedade de uvas.  

```{r read-data}
wine <- read.csv("wine.data", header = FALSE)
dados <- wine[,-1]
dados.padronizados <- scale(dados)
datatable(dados, caption = "Base de dados")
```

## Análise exploratória

```{r}
boxplot(dados,col="magenta")
```

Pelo boxplot dos dados é possível perceber que as variáveis possuem escalas bem diferentes.  

```{r}
boxplot(dados.padronizados,col="blue")
```

Já o boxplots com os dados padronizados é possível perceber que o problema de escala foi corrigido.  

```{r}
knitr::kable(summary(dados))
```
Pelo `summary` dos dados é possível perceber a diferença numérica das escalas entre as variáveis.  

```{r}
knitr::kable(cov(dados), digits = 2, caption = "Matriz de variância e covariância")
```

Na matriz de covariância é possível ver o impacto que tem as variáveis com maior escala na diagonal principal (variância).

```{r}
knitr::kable(cor(dados), digits = 2, caption = "Matriz de correlação")
plot(dados, col="red")
```



## Matriz de distância euclidiana  

### Com os dados originais  

```{r euclidean-dist}
d11=dist(dados, method = "euclidean") # matriz de distancias
m=as.matrix(d11)
# print(d11)
#print(m,digits=2)
m %>% DT::datatable(caption = "Matriz de distância euclidiana")
#knitr::kable(m, caption = "Matriz de distância euclidiana")
# Visulizando as distância entre os objetos
# vermelho - perto, azul - longe, branco - 
fviz_dist(d11,order=TRUE)
```  

### Dados padronizados  

```{r scaled-euclidean-dist}
d1=dist(dados.padronizados, method = "euclidean")
m1=as.matrix(d1)
#print(m2,digits=2)
m1 %>% DT::datatable(caption = "Matriz de distância euclidiana - dados padronizados")
fviz_dist(d1,order=TRUE)
```  

## Distância de Manhattan  

```{r scaled-manhattan-dist}
d2=dist(dados.padronizados, method = "manhattan")
m2=as.matrix(d2)
m2 %>% DT::datatable(caption = "Matriz de distância de Manhattan - dados padronizados")
#print(m2,digits=2)
fviz_dist(d2,order=TRUE)
```  

## Distância de Minkowski  

```{r scaled-minkowski-dist}
d3=dist(dados.padronizados, method = "minkowski")
m3=as.matrix(d3)
m3 %>% DT::datatable(caption = "Matriz de distância de Minkowski - dados padronizados")
#print(m3,digits=2)
fviz_dist(d3,order=TRUE)
```

## Algoritmos de Agrupamento

Os algoritmos de agrupamentos que serão abordados serão o método hierárquico, e os métodos não hierárquicos k-médias (k-means) e c-médias (c-means).


1. Hierárquico
2. k-means
3. c-means

## Método Hierárquico

```{r wardD-method}
fit=hclust(d1, method="ward.D")  # method pode ser "ward.D", "ward.D2", "single", #"complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC).
#names(fit)
#fit$labels[fit$order]

# agnes {cluster} - Computes agglomerative hierarchical clustering of the dataset.

```

### Dendrograma  

```{r dendrograma}
#par(mfrow = c(1, 2)); 
#plot(fit, hang = 0.1,cex = 0.6); 
#plot(fit, hang = -1,col="red",cex = 0.6) 

#plot(fit,col="blue",cex = 0.6)
#groups <- cutree(fit, k=4) # linha de corte k
#rect.hclust(fit, k=4, border="red")# retangulo mostrando os clusters

plot(fit, hang = 0.1,cex = 0.6); 
groups <- cutree(fit, k=2) # linha de corte k
rect.hclust(fit, k=2, border="red") # retangulo mostrando os clusters

plot(fit, hang = 0.1,cex = 0.6); 
groups <- cutree(fit, k=3) # linha de corte k
rect.hclust(fit, k=3, border="red") # retangulo mostrando os clusters

plot(fit, hang = 0.1,cex = 0.6); 
groups <- cutree(fit, k=4) # linha de corte k
rect.hclust(fit, k=4, border="red") # retangulo mostrando os clusters
```

Pela análise dos dendrogramas, o melhor número de clusters parece ser 3, pois é com 3 ramificações que a altura diminui acentuadamente.

```{r}
wss <- sapply(1:13, function(k){kmeans(dados.padronizados, centers = k)$tot.withinss})

k <- 3
modelo <- kmeans(dados.padronizados, centers = k)

fviz_nbclust(dados.padronizados, kmeans, method = "wss")+
  geom_vline(xintercept = 3, linetype = 2)
```
Esse gráfico confirma a escolha de 3 clusters. A partir do terceiro cluster a soma de quadrados passa a ter uma queda menor a medida que aumenta o número de clusters.

### Salvando o resultado
```{r}
plot.phylo(as.phylo(fit), type="p", edge.col=4, edge.width=2, 
           show.node.label=TRUE, no.margin=TRUE,cex = 0.6)
mycl <- cutree(fit, k=3)
dados.agrupados <- data.frame(wine, cluster = mycl) 
dados.agrupados[dados.agrupados$V1 != dados.agrupados$cluster,] %>% dim
```

```{r}
clust <- cutree(fit, k = 3)
fviz_cluster(list(data = dados.padronizados, cluster = clust))
```  
Pelo método hierárquico é possível perceber que alguns itens se misturam com itens de outros clusters. Sendo notável, por exemplo, o 51 que foi classificado como do cluster 2, porém está maios próximo do centro do cluster 1. 


## Método K-means  

```{r}
kmedia <- kmeans(dados.padronizados, 3) # 5 cluster solution
kmedia$centers
kmedia$size
kmedia$iter

aggregate(dados.padronizados,by <- list(kmedia$cluster),FUN=mean)
# append cluster assignment
#crimespad=data.frame(crimespad, kmedia$cluster) 
kmeans <- data.frame(wine, cluster = kmedia$cluster)
kmeans[kmeans$V1 != kmeans$cluster,] %>% dim
fviz_cluster(list(data = dados.padronizados, cluster = kmedia$cluster))
```

## Método c-means
```{r}
fuzz <- fanny(dados.padronizados, 3,memb.exp=1.5)
summary(fuzz)
plot(fuzz)

#stab <- stability(fuzz, graph = FALSE,B = 10)
si <- fuzz$silinfo
cmeans <- data.frame(wine, cluster = fuzz$clustering)
cmeans[cmeans$V1 != cmeans$cluster,] %>% dim
fviz_cluster(list(data = dados.padronizados, cluster = fuzz$clustering))

```

## PAM

Para saber o número ótimo de clusters será feito esse gráfico:   
```{r}
fviz_nbclust(dados, pam, method = "silhouette")+
  theme_classic()
```
Pela análise do gráfico nota-se que o número ótimo de cluster é 2.  

```{r}
k <- 2
pam.res <- pam(dados, k, metric = "euclidean")

names(pam.res)
pam.res$medoids

dpam <- cbind(wine, cluster = pam.res$cluster)
dpam[dpam$V1 != dpam$cluster,] %>% dim

fviz_cluster(pam.res, 
             palette = c("#00AFBB", "#FC4E07"), # color palette
             ellipse.type = "t", # Concentration ellipse
             geom = "point", pointsize = 1, main="PAM",
             ggtheme = theme_classic()
)
```
Nota-se pelo gráfico que essa clusterização não ficou tão boa, pois há elementos classificados como do cluster 1 dentro do cluster 2 e vice-versa.


## CLARA

Número de clusters para o clara:  

```{r}
fviz_nbclust(dados, clara, method = "silhouette")+
  theme_classic()
```
O número ótimo de clusters para o método CLARA também foi 2.  

```{r}
k <- 2
clara.res <- clara(dados, k, samples = 50, sampsize=120, pamLike = TRUE)

clara.res$medoids

dclara <- cbind(wine, cluster = clara.res$cluster)
dclara[dclara$V1 != dclara$cluster,] %>% dim

fviz_cluster(clara.res, 
             palette = c("#00AFBB", "#FC4E07"), # color palette
             ellipse.type = "t", # Concentration ellipse
             geom = "point", pointsize = 1,main="CLARA",
             ggtheme = theme_classic()
)

```
Pela análise do gráfico da clusterização usando o algoritmo CLARA é posível perceber o mesmo problema que acontece com o algoritmo PAM. Há pontos classificados como de um cluster na região do outro cluster.  


