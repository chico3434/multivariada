---
title: "Análise Multivariada"
subtitle: "Análise de Cluster"
author: "Francisco Rubens"
date: "2023-05-30"
output:
  rmdformats::readthedown:
    highlight: kate
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      cache=FALSE,
                      prompt=FALSE,
                      tidy=TRUE,
                      comment=NA)
```

```{r libraries}
library(ggplot2)
library(ggpubr)
library(DT)
library(mclust)
library(factoextra)
library(glue)
library(cluster)
library(fpc)
library(ape)
library(purrr)
library(mclust)
```

## Dados

Os dados se referem a uma análise química de vinhos. São 178 amostras de diferentes vinhos italianos de 3 tipos de uvas usadas na fabricação dos vinhos, tal como Sauvignon Blanc, Cabernet ou Chardonnay. Foram analisadas 13 variáveis (V2 a V14), contendo as concentrações de diferentes compostos químicos na amostra.  

As variáveis observadas para cada vinho são:  

V2. Álcool, que é criado como um resultado direto do processo de fermentação e é também um indicador do conteúdo de álcool das uvas.  
V3. Ácido málico, que é um acido primário em uvas, que podem influenciar o gosto dos vinhos.  
V4. Teor de cinzas, que são um indicador de qualidade.  
V5. Alcalinidade da cinza, uma propriedade química das cinzas.  
V6. Magnésio, um mineral.  
V7. Fenóis totais, uma classe de moléculas importantes para definir o sabor, cheiro, benefícios medicinais e diversidade do vinho. Os tipos de fenóis são classificados como flavonóides e não flavonoides.  
V8. Flavonóides, um tipo de fenol no vinho tinto que possui um maior impacto no sabor do vinho.  
V9. Fenóis não flavonóides, outro tipo de fenol.  
V10. Proantocianinas, tipo de flavonóide das semestes das uvas.  
V11. Intensidade da cor.  
V12. Tonalidade do vinho.  
V13. OD280/OD315 de vinhos diluídos  
V14. Teor de prolina, que é alterada pela variedade de uvas.  

```{r read-data}
wine <- read.csv("wine.data", header = FALSE)
dados <- wine[,-1]
dados.padronizados <- scale(dados)
DT::datatable(dados, caption = "Base de dados")
```

## Análise exploratória

```{r}
boxplot(dados,col="magenta")
```

Pelo boxplot dos dados é possível perceber que as variáveis possuem escalas bem diferentes.  

```{r}
boxplot(dados.padronizados,col="blue")
```

Já o boxplots com os dados padronizados é possível perceber que o problema de escala foi corrigido.  

```{r}
knitr::kable(summary(dados))
```
Pelo `summary` dos dados é possível perceber a diferença numérica das escalas entre as variáveis.  

```{r}
knitr::kable(cov(dados), digits = 2, caption = "Matriz de variância e covariância")
```

Na matriz de covariância é possível ver o impacto que tem as variáveis com maior escala na diagonal principal (variância).

```{r}
knitr::kable(cor(dados), digits = 2, caption = "Matriz de correlação")
plot(dados, col="red")
```



## Matriz de distância euclidiana  

### Com os dados originais  

```{r euclidean-dist}
d11=dist(dados, method = "euclidean") # matriz de distancias
m=as.matrix(d11)
# print(d11)
#print(m,digits=2)
m %>% DT::datatable(caption = "Matriz de distância euclidiana")
#knitr::kable(m, caption = "Matriz de distância euclidiana")
# Visulizando as distância entre os objetos
# vermelho - perto, azul - longe, branco - 
fviz_dist(d11,order=TRUE)
```  

### Dados padronizados  

```{r scaled-euclidean-dist}
d1=dist(dados.padronizados, method = "euclidean")
m1=as.matrix(d1)
#print(m2,digits=2)
m1 %>% DT::datatable(caption = "Matriz de distância euclidiana - dados padronizados")
fviz_dist(d1,order=TRUE)
```  

## Distância de Manhattan  

```{r scaled-manhattan-dist}
d2=dist(dados.padronizados, method = "manhattan")
m2=as.matrix(d2)
m2 %>% DT::datatable(caption = "Matriz de distância de Manhattan - dados padronizados")
#print(m2,digits=2)
fviz_dist(d2,order=TRUE)
```  

## Distância de Minkowski  

```{r scaled-minkowski-dist}
d3=dist(dados.padronizados, method = "minkowski")
m3=as.matrix(d3)
m3 %>% DT::datatable(caption = "Matriz de distância de Minkowski - dados padronizados")
#print(m3,digits=2)
fviz_dist(d3,order=TRUE)
```

## Algoritmos de Agrupamento

Os algoritmos de agrupamentos que serão abordados serão o método hierárquico, e os métodos não hierárquicos k-médias (k-means) e c-médias (c-means).


1. Hierárquico
2. k-means
3. c-means
4. PAM
5. CLARA

## Método Hierárquico

```{r wardD-method}
fit <- hclust(d1, method="ward.D")  # method pode ser "ward.D", "ward.D2", "single", #"complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC).
#names(fit)
#fit$labels[fit$order]

# agnes {cluster} - Computes agglomerative hierarchical clustering of the dataset.

```

### Dendrograma  

```{r dendrograma}
#par(mfrow = c(1, 2)); 
#plot(fit, hang = 0.1,cex = 0.6); 
#plot(fit, hang = -1,col="red",cex = 0.6) 

#plot(fit,col="blue",cex = 0.6)
#groups <- cutree(fit, k=4) # linha de corte k
#rect.hclust(fit, k=4, border="red")# retangulo mostrando os clusters

plot(fit, hang = 0.1,cex = 0.6); 
groups <- cutree(fit, k=2) # linha de corte k
rect.hclust(fit, k=2, border="red") # retangulo mostrando os clusters

plot(fit, hang = 0.1,cex = 0.6); 
groups <- cutree(fit, k=3) # linha de corte k
rect.hclust(fit, k=3, border="red") # retangulo mostrando os clusters

plot(fit, hang = 0.1,cex = 0.6); 
groups <- cutree(fit, k=4) # linha de corte k
rect.hclust(fit, k=4, border="red") # retangulo mostrando os clusters
```

Pela análise dos dendrogramas, o melhor número de clusters parece ser 3, pois é com 3 ramificações que a altura diminui acentuadamente.  

```{r}
mycl <- cutree(fit, k=3)
dados.agrupados <- data.frame(wine, cluster = mycl) 
diff.h <- dados.agrupados[dados.agrupados$V1 != dados.agrupados$cluster,] %>% dim
datatable(dados.agrupados, caption = "Agrupamento - Método hierárquico")
```

Fazendo uma comparação da clusterização com a variável do tipo de uva, em `r diff.h[1]` observações o tipo da uva foi diferente do cluster.  


```{r}
clust <- cutree(fit, k = 3)
fviz_cluster(list(data = dados.padronizados, cluster = clust))
```  

Pelo método hierárquico é possível perceber que alguns itens se misturam com itens de outros clusters. Sendo notável, por exemplo, o 51 que foi classificado como do cluster 2, porém está maios próximo do centro do cluster 1. 


## Método K-means  

### Número de clusters  

```{r}
fviz_nbclust(dados.padronizados, kmeans, method = "silhouette")+
  theme_classic()
fviz_nbclust(dados.padronizados, kmeans, method = "wss")+
  theme_classic()
fviz_nbclust(dados.padronizados, kmeans, method = "gap_stat")+
  theme_classic()
```

Pela análise dos gráficos percebe-se que o número ótimo de cluster é 3 nos 3 métodos.  

```{r}
kmedia <- kmeans(dados.padronizados, 3)
#kmedia$centers
#kmedia$size
#kmedia$iter

#aggregate(dados.padronizados,by <- list(kmedia$cluster),FUN=mean)
kmeans <- data.frame(wine, cluster = kmedia$cluster)
diff.kmeans <- kmeans[kmeans$V1 != kmeans$cluster,] %>% dim
datatable(kmeans, caption = "Agrupamento - Método k-means")
fviz_cluster(list(data = dados.padronizados, cluster = kmedia$cluster))
```
Foram necessárias `r kmedia$iter` iterações. Os 3 agrupamentos ficaram com os seguintes tamanhos, respectivamente, `r kmedia$size`.  

Pela visualização dos clusters é possível notar que nenhum elemento "entra" na região de outro cluster.  

Fazendo uma comparação da clusterização com a variável do tipo de uva, em 6 observações o tipo da uva foi diferente do cluster.  

## Método C-means

### Número de clusters  

```{r}
fviz_nbclust(dados.padronizados, fanny, method = "silhouette")+
  theme_classic()

fviz_nbclust(dados.padronizados, fanny, method = "wss")+
  theme_classic()

fviz_nbclust(dados.padronizados, fanny, method = "gap_stat")+
  theme_classic()
```

Pela análise dos gráficos percebe-se que o número ótimo de cluster é 2 nos 3 métodos.  

```{r}
fuzz <- fanny(dados.padronizados,2,memb.exp=1.5)

cmeans <- data.frame(wine, cluster = fuzz$clustering)
#diff.cmeans <- cmeans[cmeans$V1 != cmeans$cluster,] %>% dim
datatable(cmeans, caption = "Agrupamento - Método c-means")
fviz_cluster(list(data = dados.padronizados, cluster = fuzz$clustering))
```
Foram necessárias `r fuzz$convergence[1]` iterações.  
Pela visualização dos clusters é possível notar que nenhum elemento "entra" na região de outro cluster.  

Para o C-means é complicado comparar com o tipod e uva pois são 3 tipos de uvas e no C-means foram feitos 2 agrupamentos.  

## PAM

### Número de clusters  

```{r}
fviz_nbclust(dados.padronizados, pam, method = "silhouette")+
  theme_classic()
fviz_nbclust(dados.padronizados, pam, method = "wss")+
  theme_classic()
fviz_nbclust(dados.padronizados, pam, method = "gap_stat")+
  theme_classic()
```

Pela análise dos gráficos percebe-se que o número ótimo de cluster é 3 usando os 3 métodos.  

```{r}
k <- 3
pam.res <- pam(dados.padronizados, k, metric = "euclidean")

names(pam.res)
pam.res$medoids

dpam <- cbind(wine, cluster = pam.res$cluster)
diff.pam <- dpam[dpam$V1 != dpam$cluster,] %>% dim
datatable(dpam, caption = "Agrupamento - Método PAM")
fviz_cluster(pam.res, 
             palette = c("#00AFBB", "#FC4E07", "#E7B800"), # color palette
             ellipse.type = "t", # Concentration ellipse
             geom = "point", pointsize = 1, main="PAM",
             ggtheme = theme_classic()
)
```
Pela análise do gráfico é possível observar que essa clusterização não ficou tão boa, pois há elementos classificados como do cluster 1 dentro do cluster 2 e vice-versa.

Fazendo uma comparação da clusterização com a variável do tipo de uva, em `r diff.pam[1]` observações o tipo da uva foi diferente do cluster. 


## CLARA

### Número de clusters  

```{r}
fviz_nbclust(dados.padronizados, clara, method = "silhouette")+
  theme_classic()
fviz_nbclust(dados.padronizados, clara, method = "wss")+
  theme_classic()
fviz_nbclust(dados.padronizados, clara, method = "gap_stat")+
  theme_classic()
```

O número ótimo de clusters para o método CLARA também foi 3.  

```{r}
k <- 3
clara.res <- clara(dados.padronizados, k, samples = 50, sampsize=120, pamLike = TRUE)

clara.res$medoids

dclara <- cbind(wine, cluster = clara.res$cluster)
diff.clara <- dclara[dclara$V1 != dclara$cluster,] %>% dim
datatable(dclara, caption = "Agrupamento - Método CLARA")
fviz_cluster(clara.res, 
             palette = c("#00AFBB", "#FC4E07", "#E7B800"), # color palette
             ellipse.type = "t", # Concentration ellipse
             geom = "point", pointsize = 1,main="CLARA",
             ggtheme = theme_classic()
)

```
Pela análise do gráfico da clusterização usando o algoritmo CLARA é posível perceber o mesmo problema que acontece com o algoritmo PAM. Há pontos classificados como de um cluster na região do outro cluster.  

Fazendo uma comparação da clusterização com a variável do tipo de uva, em `r diff.clara[1]` observações o tipo da uva foi diferente do cluster. 

## Conclusão  

Pela análise dos resultados visuais dos algoritmos de clusterizações, o método k-means foi o que os clusters ficaram melhor divididos, sem um "invadindo" o outro. Além disso, considerando a variável do tipo de uva foi o método em que os clusters mais se aproximaram do tipo de uva.  



